{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMRC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HRXlJXomUz2dflUY7OHxEuaY1X0D9kQb",
      "authorship_tag": "ABX9TyP46C77gIrFpXctBoeh34mY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shinkai125/MRC/blob/main/CMRC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ltbu_BhEk1d",
        "outputId": "0af5dcff-d393-4cb2-ebf7-3b411b7077eb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVlbauGfFFGU",
        "outputId": "891df222-df86-4904-bcd2-08cf536987d4"
      },
      "source": [
        "import os\n",
        "path = \"/content/drive/My Drive\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab Notebooks',\n",
              " '.ipynb_checkpoints',\n",
              " 'evaluate.py',\n",
              " 'cmrc2018',\n",
              " 'chinese_roberta_wwm_ext_pytorch']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVMwrQbGIcCe",
        "outputId": "4c32eac9-33ba-4651-bb28-738bdca7bb77"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 24.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmY7YiSkI7nZ",
        "outputId": "edd226e3-8001-4d25-aa28-32862de35055"
      },
      "source": [
        "!pip install tensorflow-gpu -U"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/cc/a27e73cf8b23f2ce4bdd2b7089a42a7819ce6dd7366dceba406ddc5daa9c/tensorflow_gpu-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.1)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qXPAQVlKb4z"
      },
      "source": [
        "import argparse\n",
        "import io\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def _tokenize_chinese_chars(text):\n",
        "    \"\"\"\n",
        "    :param text: input text, unicode string\n",
        "    :return:\n",
        "        tokenized text, list\n",
        "    \"\"\"\n",
        "\n",
        "    def _is_chinese_char(cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #     https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    output = []\n",
        "    buff = \"\"\n",
        "    for char in text:\n",
        "        cp = ord(char)\n",
        "        if _is_chinese_char(cp) or char == \"=\":\n",
        "            if buff != \"\":\n",
        "                output.append(buff)\n",
        "                buff = \"\"\n",
        "            output.append(char)\n",
        "        else:\n",
        "            buff += char\n",
        "\n",
        "    if buff != \"\":\n",
        "        output.append(buff)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def _normalize(in_str):\n",
        "    \"\"\"\n",
        "    normalize the input unicode string\n",
        "    \"\"\"\n",
        "    in_str = in_str.lower()\n",
        "    sp_char = [\n",
        "        u':', u'_', u'`', u'，', u'。', u'：', u'？', u'！', u'(', u')',\n",
        "        u'“', u'”', u'；', u'’', u'《', u'》', u'……', u'·', u'、', u',',\n",
        "        u'「', u'」', u'（', u'）', u'－', u'～', u'『', u'』', '|'\n",
        "    ]\n",
        "    out_segs = []\n",
        "    for char in in_str:\n",
        "        if char in sp_char:\n",
        "            continue\n",
        "        else:\n",
        "            out_segs.append(char)\n",
        "    return ''.join(out_segs)\n",
        "\n",
        "\n",
        "def find_lcs(s1, s2):\n",
        "    \"\"\"find the longest common subsequence between s1 ans s2\"\"\"\n",
        "    m = [[0 for i in range(len(s2) + 1)] for j in range(len(s1) + 1)]\n",
        "    max_len = 0\n",
        "    p = 0\n",
        "    for i in range(len(s1)):\n",
        "        for j in range(len(s2)):\n",
        "            if s1[i] == s2[j]:\n",
        "                m[i + 1][j + 1] = m[i][j] + 1\n",
        "                if m[i + 1][j + 1] > max_len:\n",
        "                    max_len = m[i + 1][j + 1]\n",
        "                    p = i + 1\n",
        "    return s1[p - max_len:p], max_len\n",
        "\n",
        "\n",
        "def evaluate(ref_ans, pred_ans, verbose=False):\n",
        "    \"\"\"\n",
        "    ref_ans: reference answers, dict\n",
        "    pred_ans: predicted answer, dict\n",
        "    return:\n",
        "        f1_score: averaged F1 score\n",
        "        em_score: averaged EM score\n",
        "        total_count: number of samples in the reference dataset\n",
        "        skip_count: number of samples skipped in the calculation due to unknown errors\n",
        "    \"\"\"\n",
        "    f1 = 0\n",
        "    em = 0\n",
        "    total_count = 0\n",
        "    skip_count = 0\n",
        "    for document in ref_ans:\n",
        "        para = document[1].strip()\n",
        "        total_count += 1\n",
        "        query_id = document[0]\n",
        "        query_text = document[2].strip()\n",
        "        answers = document[3]\n",
        "        try:\n",
        "            prediction = pred_ans[str(query_id)]\n",
        "        except:\n",
        "            skip_count += 1\n",
        "            if verbose:\n",
        "                print(\"para: {}\".format(para))\n",
        "                print(\"query: {}\".format(query_text))\n",
        "                print(\"ref: {}\".format('#'.join(answers)))\n",
        "                print(\"Skipped\")\n",
        "                print('----------------------------')\n",
        "            continue\n",
        "        _f1 = calc_f1_score(answers, prediction)\n",
        "        f1 += _f1\n",
        "        em += calc_em_score(answers, prediction)\n",
        "        if verbose:\n",
        "            print(\"para: {}\".format(para))\n",
        "            print(\"query: {}\".format(query_text))\n",
        "            print(\"ref: {}\".format('#'.join(answers)))\n",
        "            print(\"cand: {}\".format(prediction))\n",
        "            print(\"score: {}\".format(_f1))\n",
        "            print('----------------------------')\n",
        "\n",
        "    f1_score = 100.0 * f1 / total_count\n",
        "    em_score = 100.0 * em / total_count\n",
        "    return f1_score, em_score, total_count, skip_count\n",
        "\n",
        "\n",
        "def calc_f1_score(answers, prediction):\n",
        "    f1_scores = []\n",
        "    for ans in answers:\n",
        "        ans_segs = _tokenize_chinese_chars(_normalize(ans))\n",
        "        prediction_segs = _tokenize_chinese_chars(_normalize(prediction))\n",
        "        lcs, lcs_len = find_lcs(ans_segs, prediction_segs)\n",
        "        if lcs_len == 0:\n",
        "            f1_scores.append(0)\n",
        "            continue\n",
        "        prec = 1.0 * lcs_len / len(prediction_segs)\n",
        "        rec = 1.0 * lcs_len / len(ans_segs)\n",
        "        f1 = (2 * prec * rec) / (prec + rec)\n",
        "        f1_scores.append(f1)\n",
        "    return max(f1_scores)\n",
        "\n",
        "\n",
        "def calc_em_score(answers, prediction):\n",
        "    em = 0\n",
        "    for ans in answers:\n",
        "        ans_ = _normalize(ans)\n",
        "        prediction_ = _normalize(prediction)\n",
        "        if ans_ == prediction_:\n",
        "            em = 1\n",
        "            break\n",
        "    return em\n",
        "\n",
        "\n",
        "def evaluate_predictions(ref_ans, pred_ans):\n",
        "    F1, EM, TOTAL, SKIP = evaluate(ref_ans, pred_ans, verbose=False)\n",
        "    output_result = OrderedDict()\n",
        "    output_result['F1'] = '%.3f' % F1\n",
        "    output_result['EM'] = '%.3f' % EM\n",
        "    output_result['TOTAL'] = TOTAL\n",
        "    output_result['SKIP'] = SKIP\n",
        "    return output_result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdmQShYyJCm7"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import transformers\n",
        "\n",
        "transformers.logging.set_verbosity_error()\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizerFast, TFBertModel, BertConfig\n",
        "\n",
        "\n",
        "def paragraph_selection(context, answer_text, answer_start, max_len=450):\n",
        "    \"\"\"\n",
        "    以答案为基本中心裁剪context, 并重新计算answer_start\n",
        "    :param max_len: BERT输入的最大长度\n",
        "    :param context: 段落文本\n",
        "    :param answer_text: 答案文本\n",
        "    :param answer_start: 答案开始位置\n",
        "    :return: context, answer_start\n",
        "    \"\"\"\n",
        "    standard = max_len - 30\n",
        "    standard_mid = standard // 2\n",
        "    if len(context) < standard:\n",
        "        return context, answer_start\n",
        "    answer_end = answer_start + len(answer_text)\n",
        "    if answer_end < standard:\n",
        "        return context, answer_start\n",
        "    answer_mid = (answer_start + answer_end) // 2\n",
        "    select_start = answer_mid - standard_mid\n",
        "    select_end = answer_mid + standard_mid\n",
        "    if select_start < 0:\n",
        "        select_start = 0\n",
        "    if select_end > len(context):\n",
        "        select_end = len(context)\n",
        "    context = context[select_start: select_end]\n",
        "    answer_start = context.find(answer_text)\n",
        "    if answer_start < 0:\n",
        "        print(select_start, select_end, len(context))\n",
        "    return context, answer_start\n",
        "\n",
        "\n",
        "def load_dataset(data_path):\n",
        "    with open(data_path) as f:\n",
        "        input_data = json.load(f)['data']\n",
        "\n",
        "    examples = []\n",
        "    for entry in tqdm(input_data):\n",
        "        for paragraph in entry[\"paragraphs\"]:\n",
        "            context = paragraph[\"context\"].strip()\n",
        "            try:\n",
        "                title = paragraph[\"title\"].strip()\n",
        "            except KeyError:\n",
        "                title = ''\n",
        "\n",
        "            for qa in paragraph[\"qas\"]:\n",
        "                qas_id = qa[\"id\"]\n",
        "                question = qa[\"question\"].strip()\n",
        "\n",
        "                is_impossible = False\n",
        "\n",
        "                if \"is_impossible\" in qa.keys():\n",
        "                    is_impossible = qa[\"is_impossible\"]\n",
        "\n",
        "                answer_starts = [answer[\"answer_start\"] for answer in qa.get(\"answers\", [])]\n",
        "                answers = [answer[\"text\"].strip() for answer in qa.get(\"answers\", [])]\n",
        "\n",
        "                if len(answer_starts) == 0 or answer_starts[0] == -1:\n",
        "                    examples.append({\n",
        "                        \"id\": qas_id,\n",
        "                        \"title\": title,\n",
        "                        \"context\": context,\n",
        "                        \"question\": question,\n",
        "                        \"answers\": answers,\n",
        "                        \"answer_starts\": answer_starts,\n",
        "                        \"is_impossible\": is_impossible\n",
        "                    })\n",
        "                else:\n",
        "                    answer_start = answer_starts[0]\n",
        "                    answer_text = answers[0]\n",
        "                    cut_context, answer_start = paragraph_selection(context, answer_text, answer_start, max_len=450)\n",
        "                    examples.append({\n",
        "                        \"id\": qas_id,\n",
        "                        \"title\": title,\n",
        "                        \"context\": cut_context,\n",
        "                        \"question\": question,\n",
        "                        \"answers\": [answer_text],\n",
        "                        \"answer_starts\": [answer_start],\n",
        "                        \"is_impossible\": is_impossible\n",
        "                    })\n",
        "    return examples\n",
        "\n",
        "\n",
        "def convert_to_features(examples, tokenizer, max_len=450, stride=128):\n",
        "    questions = [examples[i]['question'] for i in range(len(examples))]\n",
        "    contexts = [examples[i]['context'] for i in range(len(examples))]\n",
        "    tokenized_examples = tokenizer(questions,\n",
        "                                   contexts,\n",
        "                                   padding=\"max_length\",\n",
        "                                   max_length=max_len,\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=stride,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   return_overflowing_tokens=False)\n",
        "\n",
        "    tokenized_examples = pd.DataFrame.from_dict(tokenized_examples, orient=\"index\").T\n",
        "    tokenized_examples = tokenized_examples.to_dict(orient=\"records\")\n",
        "\n",
        "    for i, tokenized_example in enumerate(tqdm(tokenized_examples)):\n",
        "        input_ids = tokenized_example[\"input_ids\"]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        offsets = tokenized_example['offset_mapping']\n",
        "        sequence_ids = tokenized_example['token_type_ids']\n",
        "\n",
        "        answers = examples[i]['answers']\n",
        "        answer_starts = examples[i]['answer_starts']\n",
        "\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answer_starts) == 0 or answer_starts[0] == -1:\n",
        "            tokenized_examples[i][\"start_positions\"] = cls_index\n",
        "            tokenized_examples[i][\"end_positions\"] = cls_index\n",
        "            tokenized_examples[i]['answerable_label'] = 0\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answer_starts[0]\n",
        "            end_char = start_char + len(answers[0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 2\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "            token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and\n",
        "                    offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[i][\"start_positions\"] = cls_index\n",
        "                tokenized_examples[i][\"end_positions\"] = cls_index\n",
        "                tokenized_examples[i]['answerable_label'] = 0\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[i][\"start_positions\"] = token_start_index - 1\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[i][\"end_positions\"] = token_end_index + 1\n",
        "                tokenized_examples[i]['answerable_label'] = 1\n",
        "\n",
        "        tokenized_examples[i][\"example_id\"] = examples[i]['id']\n",
        "\n",
        "    dataset_dict = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": [], \"start_positions\": [],\n",
        "                    \"end_positions\": [], }\n",
        "    for item in tokenized_examples:\n",
        "        for key in dataset_dict:\n",
        "            dataset_dict[key].append(item[key])\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [dataset_dict[\"input_ids\"], dataset_dict[\"token_type_ids\"], dataset_dict[\"attention_mask\"]]\n",
        "    y = [dataset_dict[\"start_positions\"], dataset_dict[\"end_positions\"]]\n",
        "\n",
        "    return tokenized_examples, x, y\n",
        "\n",
        "\n",
        "def build_model(model_path, max_len):\n",
        "    encoder = TFBertModel.from_pretrained(model_path, from_pt=True)\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    sequence_output= encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation(keras.activations.softmax, name=\"start\")(start_logits)\n",
        "    end_probs = layers.Activation(keras.activations.softmax, name=\"end\")(end_logits)\n",
        "\n",
        "    bert_model = keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[start_probs, end_probs],\n",
        "                             name=\"BERTForQuestionAnswer\")\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = keras.optimizers.Nadam(lr=3e-5)\n",
        "    bert_model.compile(optimizer=optimizer, loss=[loss, loss], metrics=['acc'])\n",
        "    return bert_model\n",
        "\n",
        "\n",
        "def TrainEMF1(eval_examples, tokenized_examples, tokenizer):\n",
        "    em, f1, total_count = 0, 0, 0\n",
        "    count = 0\n",
        "    for idx, example in enumerate(tqdm(eval_examples, desc=\"Evaluation\")):\n",
        "        total_count += 1\n",
        "        offsets = tokenized_examples[idx]['offset_mapping']\n",
        "        start = tokenized_examples[idx][\"start_positions\"]\n",
        "        end = tokenized_examples[idx][\"end_positions\"]\n",
        "        if (start >= len(offsets) or end >= len(offsets) or offsets[start] is None or\n",
        "                offsets[end] is None or offsets[start] == (0, 0) or offsets[end] == (0, 0)):\n",
        "            prediction = \"\"\n",
        "            print(start, end, offsets[start], offsets[end])\n",
        "            print(\"\".join(tokenizer.decode(tokenized_examples[idx][\"input_ids\"])))\n",
        "            print(example[\"answers\"])\n",
        "            count += 1\n",
        "        else:\n",
        "            pred_char_start = offsets[start][0]\n",
        "            pred_char_end = offsets[end][1]\n",
        "            prediction = example[\"context\"][pred_char_start:pred_char_end]\n",
        "\n",
        "        answers = example[\"answers\"]\n",
        "        f1 += calc_f1_score(answers, prediction)\n",
        "        em += calc_em_score(answers, prediction)\n",
        "    print(count)\n",
        "    f1_score = 100.0 * f1 / (total_count - 0)\n",
        "    em_score = 100.0 * em / (total_count - 0)\n",
        "    tqdm.write(f\"F1 score={f1_score:.3f},  EM score={em_score:.3f}\")\n",
        "\n",
        "\n",
        "class EM_F1Score(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, eval_x, eval_y, eval_examples, tokenized_examples):\n",
        "        super().__init__()\n",
        "        self.x_eval = eval_x\n",
        "        self.y_eval = eval_y\n",
        "        self.eval_examples = eval_examples\n",
        "        self.tokenized_examples = tokenized_examples\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        em, f1, total_count = 0, 0, 0\n",
        "        for idx, (start, end) in enumerate(tqdm(list(zip(pred_start, pred_end)), desc=\"Evaluation\")):\n",
        "            total_count += 1\n",
        "            example = self.eval_examples[idx]\n",
        "            offsets = self.tokenized_examples[idx]['offset_mapping']\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if (start >= len(offsets) or end >= len(offsets) or offsets[start] is None or\n",
        "                    offsets[end] is None or offsets[start] == (0, 0) or offsets[end] == (0, 0)):\n",
        "                prediction = \"\"\n",
        "            else:\n",
        "                pred_char_start = offsets[start][0]\n",
        "                pred_char_end = offsets[end][1]\n",
        "                prediction = example[\"context\"][pred_char_start:pred_char_end]\n",
        "\n",
        "            answers = example[\"answers\"]\n",
        "            f1 += calc_f1_score(answers, prediction)\n",
        "            em += calc_em_score(answers, prediction)\n",
        "        f1_score = 100.0 * f1 / total_count\n",
        "        em_score = 100.0 * em / total_count\n",
        "        logs['F1'] = f1_score\n",
        "        logs['EM'] = em_score\n",
        "        tqdm.write(f\"epoch={epoch + 1}, F1 score={f1_score:.3f},  EM score={em_score:.3f}\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRNDMiAnJJ_s",
        "outputId": "3e52b99d-e675-4075-a52f-1caf573b3830"
      },
      "source": [
        "train_path = './cmrc2018/cmrc2018_train.json'\n",
        "eval_path = './cmrc2018/cmrc2018_dev.json'\n",
        "\n",
        "max_len = 450\n",
        "stride = 128\n",
        "bert_model_path = \"./chinese_roberta_wwm_ext_pytorch\"\n",
        "tokenizer = BertTokenizerFast.from_pretrained(bert_model_path)\n",
        "train_examples = load_dataset(data_path=train_path)\n",
        "train_tokenized_examples, x_train, y_train = convert_to_features(train_examples, tokenizer, max_len=max_len,\n",
        "                                                                  stride=stride)\n",
        "print(f\"{len(train_examples)} train_examples {len(train_tokenized_examples)} train_tokenized_examples.\")\n",
        "\n",
        "eval_examples = load_dataset(data_path=eval_path)\n",
        "eval_tokenized_examples, x_eval, y_eval = convert_to_features(eval_examples, tokenizer, max_len=max_len,\n",
        "                                                              stride=stride)\n",
        "print(f\"{len(eval_examples)} eval_examples {len(eval_tokenized_examples)} eval_tokenized_examples.\")\n",
        "\n",
        "# TrainEMF1(train_examples, train_tokenized_examples, tokenizer)\n",
        "# TrainEMF1(eval_examples, eval_tokenized_examples, tokenizer)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2403/2403 [00:00<00:00, 50063.39it/s]\n",
            "100%|██████████| 10142/10142 [00:00<00:00, 17573.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10142 train_examples 10142 train_tokenized_examples.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 848/848 [00:00<00:00, 37597.19it/s]\n",
            "100%|██████████| 3219/3219 [00:00<00:00, 16510.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3219 eval_examples 3219 eval_tokenized_examples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh4HwHfKJUSe",
        "outputId": "8f23bfd2-172e-4fec-d8ef-69fbe7b744b7"
      },
      "source": [
        "use_tpu = True\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        model = build_model(model_path=bert_model_path, max_len=max_len)\n",
        "else:\n",
        "    model = build_model(model_path=bert_model_path, max_len=max_len)\n",
        "EMF1_Callback = EM_F1Score(x_eval, y_eval, eval_examples, eval_tokenized_examples)\n",
        "model.fit(x_train, y_train, epochs=3, verbose=1, batch_size=32, callbacks=[EMF1_Callback])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "317/317 [==============================] - 154s 164ms/step - loss: 5.3398 - start_loss: 2.7091 - end_loss: 2.6307 - start_acc: 0.3743 - end_acc: 0.3580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|██████████| 3219/3219 [00:00<00:00, 8223.68it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch=1, F1 score=82.967,  EM score=60.205\n",
            "Epoch 2/3\n",
            "317/317 [==============================] - 52s 164ms/step - loss: 2.0179 - start_loss: 1.0528 - end_loss: 0.9652 - start_acc: 0.6560 - end_acc: 0.6686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|██████████| 3219/3219 [00:00<00:00, 9173.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch=2, F1 score=84.579,  EM score=62.908\n",
            "Epoch 3/3\n",
            "317/317 [==============================] - 52s 164ms/step - loss: 1.3884 - start_loss: 0.7348 - end_loss: 0.6535 - start_acc: 0.7429 - end_acc: 0.7549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|██████████| 3219/3219 [00:00<00:00, 9059.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch=3, F1 score=83.816,  EM score=60.671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbdc6aae110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3IuqO4uK04M"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}